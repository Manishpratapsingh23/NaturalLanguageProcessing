{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d89a855e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\manis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\manis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\manis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random   # Random number generation\n",
    "import numpy as np  # Numerical operations\n",
    "import nltk # Natural Language Toolkit for text processing\n",
    "from nltk.corpus import wordnet    # WordNet for lexical relations mainly used for synonyms\n",
    "nltk.download('wordnet')    # Download WordNet data\n",
    "nltk.download('omw-1.4') # Download Open Multilingual WordNet for translations\n",
    "nltk.download('punkt_tab') # Download Punkt tokenizer models for sentence splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a804d61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The Movie was absolutely fantastic and enjoyable\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a42a927c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_synonyms(word):\n",
    "    \"\"\"Get synonyms for a given word using WordNet.\"\"\"\n",
    "    synonyms = set()  # Use a set to avoid duplicates\n",
    "    # Iterate through all synsets of the word\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonym = lemma.name().replace('_', ' ')  # Replace underscores with spaces for readability\n",
    "            if synonym.lower() != word.lower():  # Avoid adding the original word as a synonym\n",
    "                synonyms.add(lemma.name())\n",
    "    return list(synonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60916041",
   "metadata": {},
   "outputs": [],
   "source": [
    "def synonym_replacement(text, n=2):\n",
    "    words = nltk.word_tokenize(text) # Tokenize the text into words\n",
    "    new_words = words.copy()\n",
    "    random_word_list = [word for word in new_words if word.isalpha()]  # Filter out non-alphabetic words\n",
    "    random.shuffle(random_word_list)  # Shuffle the list to randomize selection\n",
    "    num_replaced = 0\n",
    "    for word in random_word_list:\n",
    "        synonyms = get_synonyms(word)\n",
    "        if len(synonyms) >= 1:\n",
    "            synonym = random.choice(synonyms)   # Randomly select a synonym\n",
    "            new_words = [synonym if w == word else w for w in new_words]\n",
    "            num_replaced += 1\n",
    "        if num_replaced >= n:  # Stop after replacing 'n' words\n",
    "            break\n",
    "    return ' '.join(new_words)  # Join the words back into a single string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9dd504a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: The Movie was absolutely fantastic and enjoyable\n",
      "Augmented Text: The film was absolutely marvellous and enjoyable\n"
     ]
    }
   ],
   "source": [
    "original = \"The Movie was absolutely fantastic and enjoyable\"\n",
    "augmented_text = synonym_replacement(original, n=2)  # Replace 2 words with synonyms\n",
    "print(\"Original Text:\", original)\n",
    "print(\"Augmented Text:\", augmented_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cc51a575",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_filp(text):\n",
    "    words = nltk.word_tokenize(text)  # Tokenize the text into words\n",
    "    new_words = words.copy()\n",
    "    indices = list(range(len(words)-1))\n",
    "    if not indices:\n",
    "        return text\n",
    "    flip_index = random.choice(indices)  # Randomly select an index to flip\n",
    "    new_words[flip_index], new_words[flip_index+1] = new_words[flip_index+1], new_words[flip_index]  # Swap the selected word with the next one\n",
    "    return ' '.join(new_words)  # Join the words back into a single string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e6544122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: The Movie was absolutely fantastic and enjoyable\n",
      "Bigram Text: The Movie was fantastic absolutely and enjoyable\n"
     ]
    }
   ],
   "source": [
    "text2 = \"The Movie was absolutely fantastic and enjoyable\"\n",
    "bigram_text = bigram_filp(text2)  # Replace 2 words with synonyms\n",
    "print(\"Original Text:\", text2)\n",
    "print(\"Bigram Text:\", bigram_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e2e0b684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting deep_translator\n",
      "  Obtaining dependency information for deep_translator from https://files.pythonhosted.org/packages/38/3f/61a8ef73236dbea83a1a063a8af2f8e1e41a0df64f122233938391d0f175/deep_translator-1.11.4-py3-none-any.whl.metadata\n",
      "  Downloading deep_translator-1.11.4-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting beautifulsoup4<5.0.0,>=4.9.1 (from deep_translator)\n",
      "  Obtaining dependency information for beautifulsoup4<5.0.0,>=4.9.1 from https://files.pythonhosted.org/packages/50/cd/30110dc0ffcf3b131156077b90e9f60ed75711223f306da4db08eff8403b/beautifulsoup4-4.13.4-py3-none-any.whl.metadata\n",
      "  Downloading beautifulsoup4-4.13.4-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting requests<3.0.0,>=2.23.0 (from deep_translator)\n",
      "  Obtaining dependency information for requests<3.0.0,>=2.23.0 from https://files.pythonhosted.org/packages/7c/e4/56027c4a6b4ae70ca9de302488c5ca95ad4a39e190093d6c1a8ace08341b/requests-2.32.4-py3-none-any.whl.metadata\n",
      "  Downloading requests-2.32.4-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4<5.0.0,>=4.9.1->deep_translator)\n",
      "  Obtaining dependency information for soupsieve>1.2 from https://files.pythonhosted.org/packages/e7/9c/0e6afc12c269578be5c0c1c9f4b49a8d32770a080260c333ac04cc1c832d/soupsieve-2.7-py3-none-any.whl.metadata\n",
      "  Downloading soupsieve-2.7-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\manis\\appdata\\roaming\\python\\python311\\site-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep_translator) (4.12.2)\n",
      "Collecting charset_normalizer<4,>=2 (from requests<3.0.0,>=2.23.0->deep_translator)\n",
      "  Obtaining dependency information for charset_normalizer<4,>=2 from https://files.pythonhosted.org/packages/a8/05/5e33dbef7e2f773d672b6d79f10ec633d4a71cd96db6673625838a4fd532/charset_normalizer-3.4.2-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading charset_normalizer-3.4.2-cp311-cp311-win_amd64.whl.metadata (36 kB)\n",
      "Collecting idna<4,>=2.5 (from requests<3.0.0,>=2.23.0->deep_translator)\n",
      "  Obtaining dependency information for idna<4,>=2.5 from https://files.pythonhosted.org/packages/76/c6/c88e154df9c4e1a2a66ccf0005a88dfb2650c1dffb6f5ce603dfbd452ce3/idna-3.10-py3-none-any.whl.metadata\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3.0.0,>=2.23.0->deep_translator)\n",
      "  Obtaining dependency information for urllib3<3,>=1.21.1 from https://files.pythonhosted.org/packages/a7/c2/fe1e52489ae3122415c51f387e221dd0773709bad6c6cdaa599e8a2c5185/urllib3-2.5.0-py3-none-any.whl.metadata\n",
      "  Downloading urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests<3.0.0,>=2.23.0->deep_translator)\n",
      "  Obtaining dependency information for certifi>=2017.4.17 from https://files.pythonhosted.org/packages/e5/48/1549795ba7742c948d2ad169c1c8cdbae65bc450d6cd753d124b17c8cd32/certifi-2025.8.3-py3-none-any.whl.metadata\n",
      "  Downloading certifi-2025.8.3-py3-none-any.whl.metadata (2.4 kB)\n",
      "Downloading deep_translator-1.11.4-py3-none-any.whl (42 kB)\n",
      "   ---------------------------------------- 0.0/42.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 42.3/42.3 kB 2.0 MB/s eta 0:00:00\n",
      "Downloading beautifulsoup4-4.13.4-py3-none-any.whl (187 kB)\n",
      "   ---------------------------------------- 0.0/187.3 kB ? eta -:--:--\n",
      "   ------------------- -------------------- 92.2/187.3 kB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 187.3/187.3 kB 2.3 MB/s eta 0:00:00\n",
      "Downloading requests-2.32.4-py3-none-any.whl (64 kB)\n",
      "   ---------------------------------------- 0.0/64.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 64.8/64.8 kB 1.8 MB/s eta 0:00:00\n",
      "Downloading certifi-2025.8.3-py3-none-any.whl (161 kB)\n",
      "   ---------------------------------------- 0.0/161.2 kB ? eta -:--:--\n",
      "   -------------------- ------------------- 81.9/161.2 kB 2.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 161.2/161.2 kB 1.9 MB/s eta 0:00:00\n",
      "Downloading charset_normalizer-3.4.2-cp311-cp311-win_amd64.whl (105 kB)\n",
      "   ---------------------------------------- 0.0/105.4 kB ? eta -:--:--\n",
      "   ----------------------- ---------------- 61.4/105.4 kB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 105.4/105.4 kB 1.5 MB/s eta 0:00:00\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Downloading soupsieve-2.7-py3-none-any.whl (36 kB)\n",
      "Downloading urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "   ---------------------------------------- 0.0/129.8 kB ? eta -:--:--\n",
      "   ------------------------------- -------- 102.4/129.8 kB 3.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 129.8/129.8 kB 2.5 MB/s eta 0:00:00\n",
      "Installing collected packages: urllib3, soupsieve, idna, charset_normalizer, certifi, requests, beautifulsoup4, deep_translator\n",
      "Successfully installed beautifulsoup4-4.13.4 certifi-2025.8.3 charset_normalizer-3.4.2 deep_translator-1.11.4 idna-3.10 requests-2.32.4 soupsieve-2.7 urllib3-2.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install deep_translator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f9142b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_translator import GoogleTranslator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2baac3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_translate_verbose(text, intermediate_lang='fr'):\n",
    "    try:\n",
    "        translated = GoogleTranslator(source='auto' , target=intermediate_lang).translate(text)\n",
    "        back_translate = GoogleTranslator(source='auto', translated='en').translate(translated)\n",
    "\n",
    "        print(text)\n",
    "        print(translated)\n",
    "        print(back_translate)\n",
    "\n",
    "        return back_translate\n",
    "    except Exception as e:\n",
    "        print(\"Translation error:\", e)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "36a93736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Movie was absolutely fantastic and enjoyable\n",
      "Le film était absolument fantastique et agréable\n",
      "The film was absolutely fantastic and pleasant\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The film was absolutely fantastic and pleasant'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text3 = \"The Movie was absolutely fantastic and enjoyable\"\n",
    "back_translate_verbose(text3,intermediate_lang='fr')  # Replace 2 words with synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "05e31c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def add_noise(text, noise_level = 0.1):\n",
    "    text_chars = list(text)\n",
    "    num_noisy = int(len(text_chars)*noise_level)\n",
    "    for _ in range(num_noisy):\n",
    "        idx = random.randint(0,len(text_chars)-2)\n",
    "        text_chars[idx], text_chars[idx+1] = text_chars[idx+1], text_chars[idx]\n",
    "    return ''.join(text_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2f735463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Movie was absolutely afntastic ande nojybale\n"
     ]
    }
   ],
   "source": [
    "print(add_noise(text3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
